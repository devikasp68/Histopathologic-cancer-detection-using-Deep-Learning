{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf507007618542bdc0f4aca885e5855d0dec7e18"
   },
   "source": [
    "# **Histopathologic Cancer Detection**\n",
    "***Identification of Metastatic Tissue in Histopathologic Scans of Lymph Node Sections***    \n",
    "\n",
    "### **By [Soumya Ranjan Behera](https://www.linkedin.com/in/soumya044)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61aab0acf03a9c78bca4626b3b6f65936f86494b"
   },
   "source": [
    "## **Abstract**\n",
    "**Importance:**  \n",
    "Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency.\n",
    "\n",
    "**Objective:**  \n",
    "Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin–stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists’ diagnoses in a diagnostic setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9371002e1a6788f285e935acd83f96ba0e739362"
   },
   "source": [
    "## **Major Outcomes:**  \n",
    "The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic (ROC) curve analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e79bc3d5a6ee76c91bdee3ef7b891d57f5c35eae"
   },
   "source": [
    "## **About the Data Set**  \n",
    "The data for this kernel is a slightly modified version of the [PatchCamelyon (PCam)](https://github.com/basveeling/pcam) benchmark dataset. The original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates.  \n",
    "\n",
    "The PatchCamelyon benchmark is a new and challenging image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than imagenet, trainable on a single GPU.  \n",
    "\n",
    "PCam packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9891a3f0b3dc9d4e53042330e57f44eeae6b1685"
   },
   "source": [
    "**The images are labeled as 0 or 1, where 0 = No Tumor Tissue and 1 = Has Tumor Tissue(s)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:04:35.426721Z",
     "iopub.status.busy": "2022-04-22T16:04:35.426460Z",
     "iopub.status.idle": "2022-04-22T16:04:36.579432Z",
     "shell.execute_reply": "2022-04-22T16:04:36.578620Z",
     "shell.execute_reply.started": "2022-04-22T16:04:35.426672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'histopathologic-cancer-detection (1).ipynb', 'sample_submission.csv', 'sample_submission.csv.zip', 'test', 'test.zip', 'train', 'train.zip', 'train_labels.csv', 'train_labels.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"C:\\\\Users\\\\Devika\\\\Downloads\\\\data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e37caaed3fcf7ce6022845347d7ff01eb7550f3f"
   },
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Devika\\\\Downloads\\\\data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "907872e78e7c40c8ee288e3879e42f1f2ed5f8e4"
   },
   "source": [
    "### Total Samples Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:04:51.109458Z",
     "iopub.status.busy": "2022-04-22T16:04:51.109122Z",
     "iopub.status.idle": "2022-04-22T16:04:59.095969Z",
     "shell.execute_reply": "2022-04-22T16:04:59.095303Z",
     "shell.execute_reply.started": "2022-04-22T16:04:51.109415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images =  220026\n",
      "Test Images =  57458\n"
     ]
    }
   ],
   "source": [
    "# Total Samples Available\n",
    "print('Train Images = ',len(os.listdir('C:\\\\Users\\\\Devika\\\\Downloads\\\\data\\\\train')))\n",
    "print('Test Images = ',len(os.listdir('C:\\\\Users\\\\Devika\\\\Downloads\\\\data\\\\test')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ea02846b55222461342cec080b44f1261804db4"
   },
   "source": [
    "### Create a DataFrame of all Train Image Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "b862721d2db8b1aa633e19ae38cc30f8dc62d1d7",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:05:08.077620Z",
     "iopub.status.busy": "2022-04-22T16:05:08.077293Z",
     "iopub.status.idle": "2022-04-22T16:05:08.588005Z",
     "shell.execute_reply": "2022-04-22T16:05:08.586462Z",
     "shell.execute_reply.started": "2022-04-22T16:05:08.077572Z"
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\Devika\\\\Downloads\\\\data\\\\train_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2188/1453354787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Devika\\\\Downloads\\\\data\\\\train_labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Shape of DataFrame'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\Devika\\\\Downloads\\\\data\\\\train_labels.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Devika\\\\Downloads\\\\data\\\\train_labels.csv')\n",
    "print('Shape of DataFrame',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88fdfddefa4d6061df1dd24c6b37ae369e0f81b5"
   },
   "source": [
    "### **Visualize some Train Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "890ed0a67736ea238d6a5ed1b46becfba43fd178",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:05:20.008894Z",
     "iopub.status.busy": "2022-04-22T16:05:20.008518Z",
     "iopub.status.idle": "2022-04-22T16:05:20.012828Z",
     "shell.execute_reply": "2022-04-22T16:05:20.011797Z",
     "shell.execute_reply.started": "2022-04-22T16:05:20.008812Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../input/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6017cae09d2df8655b67d96d816b8f60db3854a8",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:05:24.027808Z",
     "iopub.status.busy": "2022-04-22T16:05:24.027245Z",
     "iopub.status.idle": "2022-04-22T16:05:25.617372Z",
     "shell.execute_reply": "2022-04-22T16:05:25.616627Z",
     "shell.execute_reply.started": "2022-04-22T16:05:24.027748Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,8))\n",
    "index = 1\n",
    "for i in np.random.randint(low = 0, high = df.shape[0], size = 10):\n",
    "    file = TRAIN_DIR + df.iloc[i]['id'] + '.tif'\n",
    "    img = cv2.imread(file)\n",
    "    ax = fig.add_subplot(2, 5, index)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    index = index + 1\n",
    "    color = ['green' if df.iloc[i].label == 1 else 'red'][0]\n",
    "    ax.set_title(df.iloc[i].label, fontsize = 18, color = color)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ddfbe3be86859a68acb71b0cc631fe5702552c1"
   },
   "source": [
    "### See the distribution of Train Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "adab435e9f6481bcd2009f847f64e89eab0b44fd",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:05:51.187756Z",
     "iopub.status.busy": "2022-04-22T16:05:51.187477Z",
     "iopub.status.idle": "2022-04-22T16:05:51.304019Z",
     "shell.execute_reply": "2022-04-22T16:05:51.303029Z",
     "shell.execute_reply.started": "2022-04-22T16:05:51.187708Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing this image because it caused a training error previously\n",
    "df[df['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# removing this image because it's black\n",
    "df[df['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "406180d03beea5afc5675e72150b0395db4b0829",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:05:54.757038Z",
     "iopub.status.busy": "2022-04-22T16:05:54.756728Z",
     "iopub.status.idle": "2022-04-22T16:05:54.975837Z",
     "shell.execute_reply": "2022-04-22T16:05:54.975018Z",
     "shell.execute_reply.started": "2022-04-22T16:05:54.756984Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6,6)) \n",
    "ax = sns.countplot(df.label).set_title('Label Counts', fontsize = 18)\n",
    "plt.annotate(df.label.value_counts()[0],\n",
    "            xy = (0,df.label.value_counts()[0] + 2000),\n",
    "            va = 'bottom',\n",
    "            ha = 'center',\n",
    "            fontsize = 12)\n",
    "plt.annotate(df.label.value_counts()[1],\n",
    "            xy = (1,df.label.value_counts()[1] + 2000),\n",
    "            va = 'bottom',\n",
    "            ha = 'center',\n",
    "            fontsize = 12)\n",
    "plt.ylim(0,150000)\n",
    "plt.ylabel('Count', fontsize = 16)\n",
    "plt.xlabel('Labels', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf872d7e100c23dc7256c88eca0353b754d56e8a"
   },
   "source": [
    "Here the **Label-1** is **60%** and **Label-0** is **40%** of the whole train images. There is a little imbalance here which we can rectify to get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a28db6e854cf3345e28393ca4a42738178207555"
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e76c7aae2dfd60ac61fd068d8c828b6410f75ac"
   },
   "source": [
    "**As per this [kernel](https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing) we can balance the labels using Random Sampling and reduce the memory usage or potential crash.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6cbe264f9225f0613a6587c3d8adcd9e3854cda"
   },
   "source": [
    "### **Take 80K images from both categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "67cb329de44b201e5190b81bc914669c80ac303a",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:06:04.699277Z",
     "iopub.status.busy": "2022-04-22T16:06:04.698921Z",
     "iopub.status.idle": "2022-04-22T16:06:04.770947Z",
     "shell.execute_reply": "2022-04-22T16:06:04.770028Z",
     "shell.execute_reply.started": "2022-04-22T16:06:04.699221Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 80000\n",
    "# take a random sample of class 0 with size equal to num samples in class 1\n",
    "df_0 = df[df['label'] == 0].sample(SAMPLE_SIZE, random_state = 0)\n",
    "# filter out class 1\n",
    "df_1 = df[df['label'] == 1].sample(SAMPLE_SIZE, random_state = 0)\n",
    "\n",
    "# concat the dataframes\n",
    "df_train = pd.concat([df_0, df_1], axis = 0).reset_index(drop = True)\n",
    "# shuffle\n",
    "df_train = shuffle(df_train)\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89d02968ae213d657d0fffcf548248932a8e023e"
   },
   "source": [
    "### **Split into Train and Validation Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e303dcd33773b33e43e553ca831da31f6f7be263",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:06:12.269132Z",
     "iopub.status.busy": "2022-04-22T16:06:12.268684Z",
     "iopub.status.idle": "2022-04-22T16:06:12.388609Z",
     "shell.execute_reply": "2022-04-22T16:06:12.387905Z",
     "shell.execute_reply.started": "2022-04-22T16:06:12.268934Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "# stratify=y creates a balanced validation set.\n",
    "y = df_train['label']\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.1, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab3dea5e5f64c78e7670ad162f31ea591fd96a46"
   },
   "source": [
    "### **Put the two types of images into two folder to help Keras ImageGenerator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69bbae64fa38dcc9961b0a545eb4cc9d3dbcbc9f"
   },
   "source": [
    "**Creating Directory Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6c47212e910f096dccb21e6603686218253c0b2d",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:07:48.847821Z",
     "iopub.status.busy": "2022-04-22T16:07:48.847547Z",
     "iopub.status.idle": "2022-04-22T16:07:48.856803Z",
     "shell.execute_reply": "2022-04-22T16:07:48.855736Z",
     "shell.execute_reply.started": "2022-04-22T16:07:48.847770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'base_dir7'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "\n",
    "#Folder Structure\n",
    "\n",
    "'''\n",
    "    * base_dir\n",
    "        |-- train_dir\n",
    "            |-- 0   #No Tumor\n",
    "            |-- 1   #Has Tumor\n",
    "        |-- val_dir\n",
    "            |-- 0\n",
    "            |-- 1\n",
    "'''\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "# create new folders inside train_dir\n",
    "no_tumor = os.path.join(train_dir, '0')\n",
    "os.mkdir(no_tumor)\n",
    "has_tumor = os.path.join(train_dir, '1')\n",
    "os.mkdir(has_tumor)\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "no_tumor = os.path.join(val_dir, '0')\n",
    "os.mkdir(no_tumor)\n",
    "has_tumor = os.path.join(val_dir, '1')\n",
    "os.mkdir(has_tumor)\n",
    "\n",
    "\n",
    "print(os.listdir('base_dir7/train_dir'))\n",
    "print(os.listdir('base_dir7/val_dir'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d26cfa79a1373d0eac3ee1f13a74f5b11b93df1"
   },
   "source": [
    "**Transfer the respective images into their respective folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c8ca025d9524110a511941f81bffc6a689426589",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:08:01.808749Z",
     "iopub.status.busy": "2022-04-22T16:08:01.808455Z",
     "iopub.status.idle": "2022-04-22T16:25:20.486888Z",
     "shell.execute_reply": "2022-04-22T16:25:20.486146Z",
     "shell.execute_reply.started": "2022-04-22T16:08:01.808691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the id as the index in df_data\n",
    "df.set_index('id', inplace=True)\n",
    "\n",
    "# Get a list of train and val images\n",
    "train_list = list(df_train['id'])\n",
    "val_list = list(df_val['id'])\n",
    "\n",
    "\n",
    "\n",
    "# Transfer the train images\n",
    "\n",
    "for image in train_list:\n",
    "    \n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    file_name = image + '.tif'\n",
    "    # get the label for a certain image\n",
    "    target = df.loc[image,'label']\n",
    "    \n",
    "    # these must match the folder names\n",
    "    if target == 0:\n",
    "        label = '0'\n",
    "    elif target == 1:\n",
    "        label = '1'\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join('../input/train', file_name)\n",
    "    # destination path to image\n",
    "    dest = os.path.join(train_dir, label, file_name)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "\n",
    "# Transfer the val images\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    file_name = image + '.tif'\n",
    "    # get the label for a certain image\n",
    "    target = df.loc[image,'label']\n",
    "    \n",
    "    # these must match the folder names\n",
    "    if target == 0:\n",
    "        label = '0'\n",
    "    elif target == 1:\n",
    "        label = '1'\n",
    "    \n",
    "\n",
    "    # source path to image\n",
    "    src = os.path.join('../input/train', file_name)\n",
    "    # destination path to image\n",
    "    dest = os.path.join(val_dir, label, file_name)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2cbc03cce7295ba3d74ff0e79471bfc09b8e72e3",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:31:31.482581Z",
     "iopub.status.busy": "2022-04-22T16:31:31.482314Z",
     "iopub.status.idle": "2022-04-22T16:31:31.585817Z",
     "shell.execute_reply": "2022-04-22T16:31:31.584867Z",
     "shell.execute_reply.started": "2022-04-22T16:31:31.482535Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir('base_dir7/train_dir/0')))\n",
    "print(len(os.listdir('base_dir7/train_dir/1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "79b25f0ddbd2b0e25ae068121adee9babaac24aa",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:31:51.741227Z",
     "iopub.status.busy": "2022-04-22T16:31:51.740953Z",
     "iopub.status.idle": "2022-04-22T16:32:01.875457Z",
     "shell.execute_reply": "2022-04-22T16:32:01.874641Z",
     "shell.execute_reply.started": "2022-04-22T16:31:51.741180Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "IMAGE_SIZE = 96\n",
    "train_path = 'base_dir7/train_dir'\n",
    "valid_path = 'base_dir7/val_dir'\n",
    "test_path = '../input/test'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 32 #10\n",
    "val_batch_size = 32 #10\n",
    "\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41dc501ceb24e5b1bdd25c745fd9318267959e7e"
   },
   "source": [
    "# **Create our Model (CancerNet)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "460dc2303b867c65d8597cd90fab142ddbe340e5",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:09.440795Z",
     "iopub.status.busy": "2022-04-22T16:32:09.440525Z",
     "iopub.status.idle": "2022-04-22T16:32:09.445947Z",
     "shell.execute_reply": "2022-04-22T16:32:09.445136Z",
     "shell.execute_reply.started": "2022-04-22T16:32:09.440744Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import SeparableConv2D\n",
    "from keras.layers.core import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "04bec957f0d95b3effae51c16a989b507ff687d6",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:14.085751Z",
     "iopub.status.busy": "2022-04-22T16:32:14.085476Z",
     "iopub.status.idle": "2022-04-22T16:32:14.095361Z",
     "shell.execute_reply": "2022-04-22T16:32:14.094544Z",
     "shell.execute_reply.started": "2022-04-22T16:32:14.085701Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "            \n",
    "            #initializa model\n",
    "            model = Sequential()\n",
    "            \n",
    "            inputShape = (height, width, depth)\n",
    "            \n",
    "            #Add First Layer CONV => ReLU => Pooling\n",
    "            model.add(Conv2D(filters = 32, kernel_size = (5,5), padding=\"same\", activation='relu', input_shape= inputShape))\n",
    "            model.add(Conv2D(filters = 32, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 32, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.2))\n",
    "                      \n",
    "            #Add Second Layer CONV => ReLU => Pooling\n",
    "            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.2))\n",
    "            \n",
    "            #Add Third Layer CONV => ReLU => Pooling\n",
    "            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.25))\n",
    "            \n",
    "            \n",
    "            #FC => ReLU\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(units = 500, activation = 'relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            #FC => Output\n",
    "            model.add(Dense(classes, activation='softmax'))\n",
    "            \n",
    "            model.summary()\n",
    "            \n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "0e0dffe24441e9c2916b0963f545a2f06dce484b",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:17.933424Z",
     "iopub.status.busy": "2022-04-22T16:32:17.933161Z",
     "iopub.status.idle": "2022-04-22T16:32:17.945130Z",
     "shell.execute_reply": "2022-04-22T16:32:17.944390Z",
     "shell.execute_reply.started": "2022-04-22T16:32:17.933378Z"
    }
   },
   "outputs": [],
   "source": [
    "class CancerNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        \n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "        # CONV => RELU => POOL\n",
    "        model.add(SeparableConv2D(32, (3, 3), padding=\"same\",input_shape = inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # (CONV => RELU => POOL) * 2\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # (CONV => RELU => POOL) * 3\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "712787094f0b0dcf96c3734089eef074e3e72b6d"
   },
   "source": [
    "**Specify optimizer and loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "76501728ae07bbfadf444856cb960415b0d1446b",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:22.081022Z",
     "iopub.status.busy": "2022-04-22T16:32:22.080698Z",
     "iopub.status.idle": "2022-04-22T16:32:22.460444Z",
     "shell.execute_reply": "2022-04-22T16:32:22.459846Z",
     "shell.execute_reply.started": "2022-04-22T16:32:22.080967Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net.build(width = 96, height = 96, depth = 3, classes = 2)\n",
    "#model = CancerNet.build(width = 96, height = 96, depth = 3, classes = 2)\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "#Edit:: Adagrad(lr=1e-2, decay= 1e-2/10) was used previous;y\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e1c445afba266b84b73a6310d58b15bc4fe7cb7"
   },
   "source": [
    "### **Visualize our model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "6e24f92b35b126cf4667276b9dba82ebd9342d9b",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:29.161363Z",
     "iopub.status.busy": "2022-04-22T16:32:29.161093Z",
     "iopub.status.idle": "2022-04-22T16:32:30.314778Z",
     "shell.execute_reply": "2022-04-22T16:32:30.310781Z",
     "shell.execute_reply.started": "2022-04-22T16:32:29.161315Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6747db8e3886a7d227c6530a8cc5ba75efed9f21"
   },
   "source": [
    "## **Model Architecture**(will be shown here when notebook will run, otherwise see Output Visualization Section)  \n",
    "<img src='./model.png' alt = 'Run_the_notebook_to see_model'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "6e89e85f2e2ebbdd7a4465196c215974f5658358",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:37.121324Z",
     "iopub.status.busy": "2022-04-22T16:32:37.121039Z",
     "iopub.status.idle": "2022-04-22T16:32:37.128948Z",
     "shell.execute_reply": "2022-04-22T16:32:37.127305Z",
     "shell.execute_reply.started": "2022-04-22T16:32:37.121275Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget 'https://www.kaggleusercontent.com/kf/10003609/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lla3iIArVKorEUKxxzzMxg.Ju_WeWrdCHBebCvN-AdSwFCZRJIm1Ru5gJkP-v0jz212zkjh0ojBQ1uHu7Cv7eBXHx8xrBXQHAJpdEy8TQ59Z26Onub-OkbUbWmto-FcjuRGJfFHlxnehCU0fLVB3ZTye4beLcsar4TV_VlKHOic4QP0MW7ajdUimXs09qZhpwI.oZo9D1Huxk091PMK1QJslQ/checkpoint.h5'\n",
    "# model.load_weights('checkpoint.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e73437b74bbe2883abe87b3e43cff8c3d962b56b"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a674ddc01ff2b2bacf4c32dfe5e1840fc9c8463f"
   },
   "source": [
    "### **Define LR Scheduler and Save Model Checkpoint on Maximum Validation Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "956b166c16f529c98512e3ee429980ad7e6271b6",
    "execution": {
     "iopub.execute_input": "2022-04-22T16:32:40.071163Z",
     "iopub.status.busy": "2022-04-22T16:32:40.070837Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "filepath = \"checkpoint.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose = 1, \n",
    "                             save_best_only = True, mode = 'max') #Save Best Epoch\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor = 0.5, patience = 2, verbose = 1, mode = 'max', min_lr = 0.00001)                              \n",
    "callbacks_list = [checkpoint, reduce_lr] # LR Scheduler Used here\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch = train_steps, \n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = val_steps,\n",
    "                    epochs = 11,\n",
    "                    verbose = 1,\n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbdf26a8e7e8dadda170032e835214200138374a"
   },
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "830f161dbc3ed7b78f2f77b2c7502e8f60c48ffa"
   },
   "source": [
    "### **Compare Training and Validation Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94ed027e01cbea6fcadbb7e26f94316302146b98"
   },
   "source": [
    "We can determine our epochs based on the convergence of below graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98303020b3956a521d482e5faec939247a25e7db"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab9762944fd603c2e6397f386733aca8828d096b"
   },
   "source": [
    "### **Load the saved weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b87f923b7e9b77a2fe10012f7d63fb68636363b"
   },
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "model.load_weights('checkpoint.h5')\n",
    "\n",
    "val_loss, val_acc = \\\n",
    "model.evaluate_generator(test_gen, steps=len(df_val))\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fa0a7472c5c7cd4bdf1882d4c6798e7524e0e39"
   },
   "source": [
    "### **Validate the model (Measure Model Performance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e486814e4c1ed4f8777e4f5efa5c1487056ab63"
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "predictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f32e41a752dcb071ba2b4dc79810d5bc7f105fd7"
   },
   "outputs": [],
   "source": [
    "# Put the predictions into a dataframe.\n",
    "df_preds = pd.DataFrame(predictions, columns=['no_tumor', 'has_tumor'])\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fc00f43fbf7473a8ac0abe3142f3ea36f0bb3fe"
   },
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "y_true = test_gen.classes\n",
    "\n",
    "# Get the predicted labels as probabilities\n",
    "y_pred = df_preds['has_tumor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84f67c255101f1031aff34e852a163f5e97a3b1a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "print('ROC AUC Score = ',roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cda221a6c8ff013907be72c92f318708f3895ad9"
   },
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b228c0089bd4d23e1efcabee3745faad3a8d0870"
   },
   "source": [
    "**Let's plot our ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ab8c5f0d97d592895d645d1896e496c880740bc"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='area = {:.2f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3e07a3498f0ca8c7c2be53b28c016e05410a0d0"
   },
   "source": [
    "We are getting around **0.9** ROC AUC value, which is a quite good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d460346ca9dcb69d12636a1e41a721e13803117a"
   },
   "source": [
    "## **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62dd86efa45a118cf559c8ed2f0f1bc9a3ccdb11"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# For this to work we need y_pred as binary labels not as probabilities\n",
    "y_pred_binary = predictions.argmax(axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                               cmap = 'Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "945a87b858b89305d3eeca6a75d4312eb9559cde"
   },
   "source": [
    "## **Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c600c3454c4d21890d9725e7c98b1d51e2f316d4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Generate a classification report\n",
    "\n",
    "report = classification_report(y_true, y_pred_binary, target_names = ['no_tumor', 'has_tumor'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5cb8ed2b2d4b7122fca96982e47a48b29a8fb2fd"
   },
   "source": [
    "**Recall** = The classifier's ability to detect a given class. It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).  \n",
    "**Precision** = Given a class prediction from a classifier, how likely is it to be correct? It is the number of correct positive results divided by the number of positive results predicted by the classifier.  \n",
    "**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.  \n",
    "\n",
    "More about Evaluation Metrics [here](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234)\n",
    "\n",
    "From the confusion matrix and classification report we see that our model is equally good at detecting both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a9ba0aacffba63dc4f418ad4257fdf503704afa"
   },
   "source": [
    "***Remove our base_dir to free up some memory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "529cca23e283883a3b66dee8ba41315e6c587554"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('base_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32947e3337e8df86109fb5f91fbab545fc37febd"
   },
   "source": [
    "# **Make Test Predictions for Kaggle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8470b28d58d8fba235dd2cd0c208efd19c7b7fd3"
   },
   "source": [
    "**Move the Test images into a directory 'test_dir'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87323860d2cf7fdb76fa3a1f1eb37dd31deb6908"
   },
   "outputs": [],
   "source": [
    "#Folder Structure\n",
    "\n",
    "'''\n",
    "    * test_dir\n",
    "        |-- test_images\n",
    "'''\n",
    "\n",
    "# We will be feeding test images from a folder into predict_generator().\n",
    "\n",
    "# create test_dir\n",
    "test_dir = 'test_dir'\n",
    "os.mkdir(test_dir)\n",
    "    \n",
    "# create test_images inside test_dir\n",
    "test_images = os.path.join(test_dir, 'test_images')\n",
    "os.mkdir(test_images)\n",
    "\n",
    "# check that the directory we created exists\n",
    "os.listdir('test_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5679e1bbead0a2dadfed9813b35b2b67c2fc98e3"
   },
   "outputs": [],
   "source": [
    "# Transfer the test images into image_dir\n",
    "test_list = os.listdir('../input/test')\n",
    "\n",
    "for image in test_list:    \n",
    "    fname = image\n",
    "    # source path to image\n",
    "    src = os.path.join('../input/test', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(test_images, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)\n",
    "print('Total Test Images = ',len(os.listdir('test_dir/test_images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aafa0c8c4448b1b7894f6a1363211bdd3fd80fc9"
   },
   "outputs": [],
   "source": [
    "test_path ='test_dir'\n",
    "test_gen = datagen.flow_from_directory(test_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "573552072125f0b853626484e60cc8c3454a4a8d"
   },
   "outputs": [],
   "source": [
    "num_test_images = 57458 #len(os.listdir('test_dir/test_images')\n",
    "\n",
    "predictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f02f0edef530aed42e2752fc13b07fa7ad3feae4"
   },
   "outputs": [],
   "source": [
    "if predictions.shape[0] == num_test_images:\n",
    "    print('All Predictions Done!')\n",
    "else:\n",
    "    print('Error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0743dc8a0144451ce1f612ac456fd917d05b32e0"
   },
   "outputs": [],
   "source": [
    "# Put the predictions into a dataframe\n",
    "df_preds = pd.DataFrame(predictions, columns=['no_tumor', 'has_tumor'])\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33d5c559e5e757ddf76658d5c7c13b1109be2edd"
   },
   "source": [
    "**Extract ID field from Test Image file names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3521c8ad4f4870a29e3cef1afc9d791c354c3185"
   },
   "outputs": [],
   "source": [
    "# This outputs the file names in the sequence in which the generator processed the test images.\n",
    "test_filenames = test_gen.filenames\n",
    "\n",
    "# add the filenames to the dataframe\n",
    "df_preds['file_names'] = test_filenames\n",
    "\n",
    "# Create an id column\n",
    "# A file name now has this format: \n",
    "# images/00006537328c33e284c973d7b39d340809f7271b.tif\n",
    "\n",
    "# This function will extract the id:\n",
    "# 00006537328c33e284c973d7b39d340809f7271b\n",
    "def extract_id(x):\n",
    "    \n",
    "    # split into a list\n",
    "    a = x.split('/')\n",
    "    # split into a list\n",
    "    b = a[1].split('.')\n",
    "    extracted_id = b[0]\n",
    "    \n",
    "    return extracted_id\n",
    "\n",
    "df_preds['id'] = df_preds['file_names'].apply(extract_id)\n",
    "\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28f6cf69c8b269c675188259cea0614f02a838b1"
   },
   "outputs": [],
   "source": [
    "# Get the predicted labels.\n",
    "# We were asked to predict a probability that the image has tumor tissue\n",
    "y_pred = df_preds['has_tumor']\n",
    "\n",
    "# get the id column\n",
    "image_id = df_preds['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e5563d235627c43531a2c6f4aa13269e103d956"
   },
   "source": [
    "### **Make Submission File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "931553470211aaa720fbc288c4714f78624c05b7"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':image_id, \n",
    "                           'label':y_pred, \n",
    "                          }).set_index('id')\n",
    "\n",
    "submission.to_csv('submission.csv', columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d17d9f9c150d3aa2471d6dc23a8e38090eccd26"
   },
   "source": [
    "**Submission File :** [Download Link](./submission.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca94a0c19aa76b904a5bc404c87cb652ca0e2d66"
   },
   "source": [
    "***Remove the test_dir to free up memory and commit our kernel successfully!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78405611b84db8de39153ab22869092d4db20dd1"
   },
   "outputs": [],
   "source": [
    "# Delete the test_dir directory we created to prevent a Kaggle error.\n",
    "# Kaggle allows a max of 500 files to be saved.\n",
    "\n",
    "shutil.rmtree('test_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51e50bbb44eb921337585b421c1317a7decc1472"
   },
   "source": [
    "# **Conclusion**  \n",
    "**The proposed deep learning model works good under Kaggle environments. But we can use other deeper or pre-trained models with higher availability of resources.**  \n",
    "**These deep learning models may have achieved better diagnostic performance than real pathologists, but this will require evaluation in a clinical setting to measure its utility in diagnosis of lymph node metastases in tissue sections**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90bb83c0995988e731f555f75946734e09010096"
   },
   "source": [
    "# **References:**  \n",
    "1. B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. \"Rotation Equivariant CNNs for Digital Pathology\". [arXiv:1806.03962](http://arxiv.org/abs/1806.03962)\n",
    "\n",
    "2.  Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199–2210. [doi:jama.2017.14585](https://doi.org/10.1001/jama.2017.14585)\n",
    "\n",
    "3. [Marsh's Kernel \" how-to-use-160-000-images-without-crashing \" ](https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing)\n",
    "\n",
    "4. [Breast cancer classification with Keras and Deep Learning (CancerNet Architecture) by Adrian Rosebrock](https://www.pyimagesearch.com/2019/02/18/breast-cancer-classification-with-keras-and-deep-learning/)\n",
    "\n",
    "5. Original Data Set [PatchCamelyon (PCam) ](https://www.kaggle.com/c/histopathologic-cancer-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b880dfbeddc3fe62f1c4e1022bfc7b649409f3bb"
   },
   "source": [
    "### **Author: [Soumya Ranjan Behera](https://www.linkedin.com/in/soumya044)**   \n",
    "Feel free to connect [LinkedIn](https://www.linkedin.com/in/soumya044)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
